{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MiniduTH/BuildMasterWEB/blob/main/BulkDataExtractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code can use to efficiently extract and process large volumes of data in batches using parallel HTTP requests and HTML parsing."
      ],
      "metadata": {
        "id": "aIybiYFmDrJd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7MBJYNEPiyh8"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Function to make a POST request and return the result\n",
        "def fetch_result(index_number):\n",
        "    # url = \"replace here with the url of the site\"\n",
        "    payload = {\n",
        "        # replace the payload of the post re\n",
        "    }\n",
        "    try:\n",
        "        response = requests.post(url, data=payload)\n",
        "        return {\n",
        "            # add attributes of the objects of payload\n",
        "            # like\"indexNumber\": index_number\n",
        "        }\n",
        "    except requests.RequestException as e:\n",
        "        return {\n",
        "            \"indexNumber\": index_number,\n",
        "            \"status_code\": \"Error\",\n",
        "            \"response\": str(e)\n",
        "        }\n",
        "\n",
        "# Function to extract data from HTML response\n",
        "def extract_data_from_html(html_content):\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # Check if the \"Index Number\" text is present before proceeding\n",
        "    index_number_tag = soup.find(text=\"Index Number\")\n",
        "    if index_number_tag is None:\n",
        "        return None  # Return None if \"Index Number\" is not found\n",
        "\n",
        "    # Extract each field from the HTML content\n",
        "    examination = index_number_tag.find_next('td').find_next('td').text.strip()\n",
        "    # replace with the fields you want to extract and save it in a var\n",
        "\n",
        "\n",
        "    exam_grade = results  # Assuming the Exam Grade is the same as the result\n",
        "\n",
        "    return [\n",
        "        #return whatever you saved in vars\n",
        "    ]\n",
        "\n",
        "# Parallel fetching results\n",
        "# because we have to check a large range of data\n",
        "def fetch_all_results(start, end, batch_size=5000, max_workers=500):\n",
        "    all_results = []\n",
        "    for batch_start in range(start, end, batch_size):\n",
        "        batch_end = min(batch_start + batch_size, end)\n",
        "        results = []\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            futures = {executor.submit(fetch_result, i): i for i in range(batch_start, batch_end)}\n",
        "            for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Fetching batch {batch_start}-{batch_end}\"):\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    results.append(result)\n",
        "                except Exception as e:\n",
        "                    results.append({\n",
        "                        \"indexNumber\": futures[future],\n",
        "                        \"status_code\": \"Error\",\n",
        "                        \"response\": str(e)\n",
        "                    })\n",
        "        all_results.extend(results)\n",
        "        # Save progress after each batch\n",
        "        df = pd.DataFrame(results)\n",
        "        df.to_excel(f\"results_{batch_start}_{batch_end}.xlsx\", index=False)\n",
        "    return all_results\n",
        "\n",
        "# Main function to run the process\n",
        "def main():\n",
        "    start_time = time.time()\n",
        "\n",
        "    # start_index = x\n",
        "    # end_index = y  # Adjust with the inputs you want to check\n",
        "    results = fetch_all_results(start_index, end_index, batch_size=5000, max_workers=50)  # Adjust max_workers based on your system\n",
        "\n",
        "    # Save all results to a single file\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_excel(\"final_results.xlsx\", index=False)\n",
        "\n",
        "    # Define the desired headers for the new DataFrame\n",
        "    headers = [\n",
        "        \"x\", \"y\"]  # Replace with the actual headers you want to extract\n",
        "\n",
        "    # Create an empty list to store the extracted data\n",
        "    data = []\n",
        "\n",
        "    # Loop through each row in the DataFrame and extract data\n",
        "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Extracting data\"):\n",
        "        html_content = row['response']\n",
        "        extracted_data = extract_data_from_html(html_content)\n",
        "        if extracted_data is not None:  # Append only if data was extracted\n",
        "            data.append(extracted_data)\n",
        "\n",
        "    # Create a new DataFrame with the extracted data\n",
        "    extracted_df = pd.DataFrame(data, columns=headers)\n",
        "    extracted_df.to_excel('extracted_results.xlsx', index=False)\n",
        "\n",
        "\n",
        "#below code is added just to know how much extracted during a timeperiod. this is not necessry to run\n",
        "\n",
        "    print(f\"Data extraction complete. Saved to 'extracted_results.xlsx'.\")\n",
        "    print(f\"Elapsed time: {time.time() - start_time} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udoUboB9pJ9b"
      },
      "source": [
        "Run below code to convert excel to a  JSON file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "914itPaz0qPH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('extracted_results.xlsx')\n",
        "\n",
        "json_data = df.to_json(orient='records')\n",
        "\n",
        "with open('output.json', 'w') as f:\n",
        "    f.write(json_data)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1coJ3Mrn_K_dK-3EyCst0slMOTEVbNqBg",
      "authorship_tag": "ABX9TyNJsFgC3OYBqqM0R1qKG9fi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}